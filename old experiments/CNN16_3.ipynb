{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n",
      "83862 44057\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 20, 20, 20, 3)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 20, 20, 20, 64)    5248      \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 20, 20, 20, 32)    55328     \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 20, 20, 20, 32)    27680     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 10, 10, 10, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 10, 10, 10, 32)    27680     \n",
      "_________________________________________________________________\n",
      "conv3d_13 (Conv3D)           (None, 10, 10, 10, 16)    13840     \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 10, 10, 10, 16)    6928      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 5, 5, 5, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 5, 5, 5, 16)       6928      \n",
      "_________________________________________________________________\n",
      "conv3d_16 (Conv3D)           (None, 5, 5, 5, 8)        3464      \n",
      "_________________________________________________________________\n",
      "conv3d_17 (Conv3D)           (None, 5, 5, 5, 4)        868       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 2, 2, 2, 4)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2, 2, 2, 4)        16        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 80)                20560     \n",
      "=================================================================\n",
      "Total params: 176,988\n",
      "Trainable params: 176,980\n",
      "Non-trainable params: 8\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "209/209 [==============================] - 100s 478ms/step - loss: 0.6912 - acc: 0.5457\n",
      "Epoch 2/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.6545 - acc: 0.6360\n",
      "Epoch 3/100\n",
      "209/209 [==============================] - 101s 484ms/step - loss: 0.5928 - acc: 0.7119\n",
      "Epoch 4/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.5495 - acc: 0.7635\n",
      "Epoch 5/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.5085 - acc: 0.8018\n",
      "Epoch 6/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.4773 - acc: 0.8257\n",
      "Epoch 7/100\n",
      "209/209 [==============================] - 101s 486ms/step - loss: 0.4550 - acc: 0.8413\n",
      "Epoch 8/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.4378 - acc: 0.8520\n",
      "Epoch 9/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.4239 - acc: 0.8594\n",
      "Epoch 10/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.4157 - acc: 0.8633\n",
      "Epoch 11/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.4110 - acc: 0.8644\n",
      "Epoch 12/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.4055 - acc: 0.8663\n",
      "Epoch 13/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.4009 - acc: 0.8679\n",
      "Epoch 14/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3974 - acc: 0.8687\n",
      "Epoch 15/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3939 - acc: 0.8695\n",
      "Epoch 16/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3904 - acc: 0.8704\n",
      "Epoch 17/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3893 - acc: 0.8703\n",
      "Epoch 18/100\n",
      "209/209 [==============================] - 102s 488ms/step - loss: 0.3866 - acc: 0.8707\n",
      "Epoch 19/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3841 - acc: 0.8713\n",
      "Epoch 20/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3815 - acc: 0.8721\n",
      "Epoch 21/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3785 - acc: 0.8729\n",
      "Epoch 22/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3763 - acc: 0.8736\n",
      "Epoch 23/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3769 - acc: 0.8727\n",
      "Epoch 24/100\n",
      "209/209 [==============================] - 101s 486ms/step - loss: 0.3771 - acc: 0.8720\n",
      "Epoch 25/100\n",
      "209/209 [==============================] - 102s 488ms/step - loss: 0.3760 - acc: 0.8722\n",
      "Epoch 26/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3738 - acc: 0.8727\n",
      "Epoch 27/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3741 - acc: 0.8724\n",
      "Epoch 28/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3727 - acc: 0.8725\n",
      "Epoch 29/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3713 - acc: 0.8728\n",
      "Epoch 30/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3709 - acc: 0.8727\n",
      "Epoch 31/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3707 - acc: 0.8725\n",
      "Epoch 32/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3697 - acc: 0.8725\n",
      "Epoch 33/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3693 - acc: 0.8726\n",
      "Epoch 34/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3686 - acc: 0.8726\n",
      "Epoch 35/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3677 - acc: 0.8726\n",
      "Epoch 36/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3668 - acc: 0.8727\n",
      "Epoch 37/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3664 - acc: 0.8727\n",
      "Epoch 38/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3654 - acc: 0.8728\n",
      "Epoch 39/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3646 - acc: 0.8729\n",
      "Epoch 40/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3635 - acc: 0.8731\n",
      "Epoch 41/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3626 - acc: 0.8733\n",
      "Epoch 42/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3615 - acc: 0.8735\n",
      "Epoch 43/100\n",
      "209/209 [==============================] - 102s 488ms/step - loss: 0.3596 - acc: 0.8738\n",
      "Epoch 44/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3583 - acc: 0.8742\n",
      "Epoch 45/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3568 - acc: 0.8747\n",
      "Epoch 46/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3555 - acc: 0.8750\n",
      "Epoch 47/100\n",
      "209/209 [==============================] - 102s 488ms/step - loss: 0.3598 - acc: 0.8729\n",
      "Epoch 48/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3589 - acc: 0.8730\n",
      "Epoch 49/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3578 - acc: 0.8734\n",
      "Epoch 50/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3561 - acc: 0.8738\n",
      "Epoch 51/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3548 - acc: 0.8744\n",
      "Epoch 52/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3570 - acc: 0.8731\n",
      "Epoch 53/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3565 - acc: 0.8732\n",
      "Epoch 54/100\n",
      "209/209 [==============================] - 101s 484ms/step - loss: 0.3546 - acc: 0.8737\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3549 - acc: 0.8733\n",
      "Epoch 56/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3542 - acc: 0.8734\n",
      "Epoch 57/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3544 - acc: 0.8732\n",
      "Epoch 58/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3530 - acc: 0.8734\n",
      "Epoch 59/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3527 - acc: 0.8736\n",
      "Epoch 60/100\n",
      "209/209 [==============================] - 102s 488ms/step - loss: 0.3529 - acc: 0.8731\n",
      "Epoch 61/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3521 - acc: 0.8732\n",
      "Epoch 62/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3506 - acc: 0.8737\n",
      "Epoch 63/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3487 - acc: 0.8744\n",
      "Epoch 64/100\n",
      "209/209 [==============================] - 102s 488ms/step - loss: 0.3502 - acc: 0.8737\n",
      "Epoch 65/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3494 - acc: 0.8736\n",
      "Epoch 66/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3492 - acc: 0.8735\n",
      "Epoch 67/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3495 - acc: 0.8733\n",
      "Epoch 68/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3476 - acc: 0.8740\n",
      "Epoch 69/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3466 - acc: 0.8745\n",
      "Epoch 70/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3454 - acc: 0.8752\n",
      "Epoch 71/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3456 - acc: 0.8746\n",
      "Epoch 72/100\n",
      "209/209 [==============================] - 101s 486ms/step - loss: 0.3461 - acc: 0.8738\n",
      "Epoch 73/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3468 - acc: 0.8735\n",
      "Epoch 74/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3446 - acc: 0.8741\n",
      "Epoch 75/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3451 - acc: 0.8737\n",
      "Epoch 76/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3446 - acc: 0.8739\n",
      "Epoch 77/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3440 - acc: 0.8740\n",
      "Epoch 78/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3437 - acc: 0.8739\n",
      "Epoch 79/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3430 - acc: 0.8739\n",
      "Epoch 80/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3431 - acc: 0.8737\n",
      "Epoch 81/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3428 - acc: 0.8738\n",
      "Epoch 82/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3414 - acc: 0.8742\n",
      "Epoch 83/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3396 - acc: 0.8750\n",
      "Epoch 84/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3401 - acc: 0.8748\n",
      "Epoch 85/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3408 - acc: 0.8741\n",
      "Epoch 86/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3397 - acc: 0.8744\n",
      "Epoch 87/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3398 - acc: 0.8741\n",
      "Epoch 88/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3388 - acc: 0.8746\n",
      "Epoch 89/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3372 - acc: 0.8753\n",
      "Epoch 90/100\n",
      "209/209 [==============================] - 102s 488ms/step - loss: 0.3358 - acc: 0.8764\n",
      "Epoch 91/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3360 - acc: 0.8763\n",
      "Epoch 92/100\n",
      "209/209 [==============================] - 102s 488ms/step - loss: 0.3376 - acc: 0.8746\n",
      "Epoch 93/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3371 - acc: 0.8747\n",
      "Epoch 94/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3350 - acc: 0.8757\n",
      "Epoch 95/100\n",
      "209/209 [==============================] - 102s 487ms/step - loss: 0.3350 - acc: 0.8760\n",
      "Epoch 96/100\n",
      "209/209 [==============================] - 101s 484ms/step - loss: 0.3359 - acc: 0.8747\n",
      "Epoch 97/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3358 - acc: 0.8748\n",
      "Epoch 98/100\n",
      "209/209 [==============================] - 102s 486ms/step - loss: 0.3344 - acc: 0.8754\n",
      "Epoch 99/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3328 - acc: 0.8762\n",
      "Epoch 100/100\n",
      "209/209 [==============================] - 101s 485ms/step - loss: 0.3315 - acc: 0.8770\n",
      "(44000, 80)\n",
      "(44000, 2, 2, 20)\n",
      "(44057, 2, 2, 20)\n",
      "CNN: \n",
      "\n",
      "level 0\n",
      "[[40273     0]\n",
      " [ 3727     0]] 0.92 0.5\n",
      "[[39956     0]\n",
      " [ 4044     0]] 0.91 0.5\n",
      "[[40296     0]\n",
      " [ 3704     0]] 0.92 0.5\n",
      "[[40042     0]\n",
      " [ 3958     0]] 0.91 0.5\n",
      "level 1\n",
      "[[40006     0]\n",
      " [ 3994     0]] 0.91 0.5\n",
      "[[39813     0]\n",
      " [ 4187     0]] 0.9 0.5\n",
      "[[40134     0]\n",
      " [ 3866     0]] 0.91 0.5\n",
      "[[39935     0]\n",
      " [ 4065     0]] 0.91 0.5\n",
      "level 2\n",
      "[[39877     0]\n",
      " [ 4123     0]] 0.91 0.5\n",
      "[[39762     0]\n",
      " [ 4238     0]] 0.9 0.5\n",
      "[[40074     0]\n",
      " [ 3926     0]] 0.91 0.5\n",
      "[[39866     0]\n",
      " [ 4134     0]] 0.91 0.5\n",
      "level 3\n",
      "[[39721     0]\n",
      " [ 4279     0]] 0.9 0.5\n",
      "[[39649     0]\n",
      " [ 4351     0]] 0.9 0.5\n",
      "[[40036     0]\n",
      " [ 3964     0]] 0.91 0.5\n",
      "[[39787     0]\n",
      " [ 4213     0]] 0.9 0.5\n",
      "level 4\n",
      "[[39577     0]\n",
      " [ 4423     0]] 0.9 0.5\n",
      "[[39518     0]\n",
      " [ 4482     0]] 0.9 0.5\n",
      "[[39916     0]\n",
      " [ 4084     0]] 0.91 0.5\n",
      "[[39720     0]\n",
      " [ 4280     0]] 0.9 0.5\n",
      "level 5\n",
      "[[39537     0]\n",
      " [ 4463     0]] 0.9 0.5\n",
      "[[39377     3]\n",
      " [ 4616     4]] 0.9 0.5\n",
      "[[39719     0]\n",
      " [ 4281     0]] 0.9 0.5\n",
      "[[39626     0]\n",
      " [ 4374     0]] 0.9 0.5\n",
      "level 6\n",
      "[[39310     2]\n",
      " [ 4688     0]] 0.89 0.5\n",
      "[[39336     0]\n",
      " [ 4664     0]] 0.89 0.5\n",
      "[[39603     0]\n",
      " [ 4397     0]] 0.9 0.5\n",
      "[[39499     0]\n",
      " [ 4501     0]] 0.9 0.5\n",
      "level 7\n",
      "[[39068     0]\n",
      " [ 4932     0]] 0.89 0.5\n",
      "[[39107     0]\n",
      " [ 4893     0]] 0.89 0.5\n",
      "[[39472     1]\n",
      " [ 4524     3]] 0.9 0.5\n",
      "[[39184     0]\n",
      " [ 4816     0]] 0.89 0.5\n",
      "level 8\n",
      "[[38829     6]\n",
      " [ 5149    16]] 0.88 0.5\n",
      "[[38884     0]\n",
      " [ 5116     0]] 0.88 0.5\n",
      "[[39307     0]\n",
      " [ 4693     0]] 0.89 0.5\n",
      "[[38911     0]\n",
      " [ 5089     0]] 0.88 0.5\n",
      "level 9\n",
      "[[38519     3]\n",
      " [ 5475     3]] 0.88 0.5\n",
      "[[38525    18]\n",
      " [ 5457     0]] 0.88 0.5\n",
      "[[39169     0]\n",
      " [ 4831     0]] 0.89 0.5\n",
      "[[38673     0]\n",
      " [ 5327     0]] 0.88 0.5\n",
      "level 10\n",
      "[[38237     4]\n",
      " [ 5752     7]] 0.87 0.5\n",
      "[[38212    20]\n",
      " [ 5761     7]] 0.87 0.5\n",
      "[[38767     1]\n",
      " [ 5232     0]] 0.88 0.5\n",
      "[[38262     1]\n",
      " [ 5737     0]] 0.87 0.5\n",
      "level 11\n",
      "[[37824    30]\n",
      " [ 6101    45]] 0.86 0.5\n",
      "[[37901     4]\n",
      " [ 6094     1]] 0.86 0.5\n",
      "[[38318    24]\n",
      " [ 5651     7]] 0.87 0.5\n",
      "[[37782     1]\n",
      " [ 6217     0]] 0.86 0.5\n",
      "level 12\n",
      "[[37343    11]\n",
      " [ 6632    14]] 0.85 0.5\n",
      "[[37487    87]\n",
      " [ 6371    55]] 0.85 0.5\n",
      "[[37865    12]\n",
      " [ 6122     1]] 0.86 0.5\n",
      "[[37401     0]\n",
      " [ 6599     0]] 0.85 0.5\n",
      "level 13\n",
      "[[37032    27]\n",
      " [ 6911    30]] 0.84 0.5\n",
      "[[37146    16]\n",
      " [ 6834     4]] 0.84 0.5\n",
      "[[37451     0]\n",
      " [ 6549     0]] 0.85 0.5\n",
      "[[37082     0]\n",
      " [ 6916     2]] 0.84 0.5\n",
      "level 14\n",
      "[[36664    62]\n",
      " [ 7230    44]] 0.83 0.5\n",
      "[[36775    54]\n",
      " [ 7145    26]] 0.84 0.5\n",
      "[[37118    20]\n",
      " [ 6846    16]] 0.84 0.5\n",
      "[[36731    29]\n",
      " [ 7216    24]] 0.84 0.5\n",
      "level 15\n",
      "[[36458   101]\n",
      " [ 7381    60]] 0.83 0.5\n",
      "[[36322    23]\n",
      " [ 7648     7]] 0.83 0.5\n",
      "[[36653    67]\n",
      " [ 7260    20]] 0.83 0.5\n",
      "[[36418    24]\n",
      " [ 7553     5]] 0.83 0.5\n",
      "level 16\n",
      "[[36242    13]\n",
      " [ 7738     7]] 0.82 0.5\n",
      "[[36054   150]\n",
      " [ 7705    91]] 0.82 0.5\n",
      "[[36518    64]\n",
      " [ 7364    54]] 0.83 0.5\n",
      "[[36124    17]\n",
      " [ 7855     4]] 0.82 0.5\n",
      "level 17\n",
      "[[36000   174]\n",
      " [ 7703   123]] 0.82 0.51\n",
      "[[35863    36]\n",
      " [ 8078    23]] 0.82 0.5\n",
      "[[36316     9]\n",
      " [ 7671     4]] 0.83 0.5\n",
      "[[36000    56]\n",
      " [ 7920    24]] 0.82 0.5\n",
      "level 18\n",
      "[[35861   106]\n",
      " [ 7960    73]] 0.82 0.5\n",
      "[[35544   127]\n",
      " [ 8278    51]] 0.81 0.5\n",
      "[[36233    54]\n",
      " [ 7698    15]] 0.82 0.5\n",
      "[[35796    17]\n",
      " [ 8186     1]] 0.81 0.5\n",
      "level 19\n",
      "[[35643   172]\n",
      " [ 8093    92]] 0.81 0.5\n",
      "[[35314   177]\n",
      " [ 8416    93]] 0.8 0.5\n",
      "[[35925    10]\n",
      " [ 8059     6]] 0.82 0.5\n",
      "[[35730     4]\n",
      " [ 8265     1]] 0.81 0.5\n",
      "(44000, 2, 2, 20)\n",
      "\n",
      " BASELINE MODEL: \n",
      "\n",
      "level 0\n",
      "[[40273     0]\n",
      " [ 3727     0]] 0.92 0.5\n",
      "[[39956     0]\n",
      " [ 4044     0]] 0.91 0.5\n",
      "[[40296     0]\n",
      " [ 3704     0]] 0.92 0.5\n",
      "[[40042     0]\n",
      " [ 3958     0]] 0.91 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 1\n",
      "[[40006     0]\n",
      " [ 3994     0]] 0.91 0.5\n",
      "[[39813     0]\n",
      " [ 4187     0]] 0.9 0.5\n",
      "[[40134     0]\n",
      " [ 3866     0]] 0.91 0.5\n",
      "[[39935     0]\n",
      " [ 4065     0]] 0.91 0.5\n",
      "level 2\n",
      "[[39877     0]\n",
      " [ 4123     0]] 0.91 0.5\n",
      "[[39762     0]\n",
      " [ 4238     0]] 0.9 0.5\n",
      "[[40074     0]\n",
      " [ 3926     0]] 0.91 0.5\n",
      "[[39866     0]\n",
      " [ 4134     0]] 0.91 0.5\n",
      "level 3\n",
      "[[39721     0]\n",
      " [ 4279     0]] 0.9 0.5\n",
      "[[39649     0]\n",
      " [ 4351     0]] 0.9 0.5\n",
      "[[40036     0]\n",
      " [ 3964     0]] 0.91 0.5\n",
      "[[39787     0]\n",
      " [ 4213     0]] 0.9 0.5\n",
      "level 4\n",
      "[[39577     0]\n",
      " [ 4423     0]] 0.9 0.5\n",
      "[[39518     0]\n",
      " [ 4482     0]] 0.9 0.5\n",
      "[[39916     0]\n",
      " [ 4084     0]] 0.91 0.5\n",
      "[[39720     0]\n",
      " [ 4280     0]] 0.9 0.5\n",
      "level 5\n",
      "[[39537     0]\n",
      " [ 4463     0]] 0.9 0.5\n",
      "[[39380     0]\n",
      " [ 4620     0]] 0.9 0.5\n",
      "[[39719     0]\n",
      " [ 4281     0]] 0.9 0.5\n",
      "[[39626     0]\n",
      " [ 4374     0]] 0.9 0.5\n",
      "level 6\n",
      "[[39312     0]\n",
      " [ 4688     0]] 0.89 0.5\n",
      "[[39336     0]\n",
      " [ 4664     0]] 0.89 0.5\n",
      "[[39603     0]\n",
      " [ 4397     0]] 0.9 0.5\n",
      "[[39499     0]\n",
      " [ 4501     0]] 0.9 0.5\n",
      "level 7\n",
      "[[39068     0]\n",
      " [ 4932     0]] 0.89 0.5\n",
      "[[39107     0]\n",
      " [ 4893     0]] 0.89 0.5\n",
      "[[39473     0]\n",
      " [ 4527     0]] 0.9 0.5\n",
      "[[39184     0]\n",
      " [ 4816     0]] 0.89 0.5\n",
      "level 8\n",
      "[[38835     0]\n",
      " [ 5165     0]] 0.88 0.5\n",
      "[[38884     0]\n",
      " [ 5116     0]] 0.88 0.5\n",
      "[[39307     0]\n",
      " [ 4693     0]] 0.89 0.5\n",
      "[[38911     0]\n",
      " [ 5089     0]] 0.88 0.5\n",
      "level 9\n",
      "[[38522     0]\n",
      " [ 5478     0]] 0.88 0.5\n",
      "[[38543     0]\n",
      " [ 5457     0]] 0.88 0.5\n",
      "[[39169     0]\n",
      " [ 4831     0]] 0.89 0.5\n",
      "[[38673     0]\n",
      " [ 5327     0]] 0.88 0.5\n",
      "level 10\n",
      "[[38241     0]\n",
      " [ 5759     0]] 0.87 0.5\n",
      "[[38232     0]\n",
      " [ 5768     0]] 0.87 0.5\n",
      "[[38768     0]\n",
      " [ 5232     0]] 0.88 0.5\n",
      "[[38263     0]\n",
      " [ 5737     0]] 0.87 0.5\n",
      "level 11\n",
      "[[37854     0]\n",
      " [ 6146     0]] 0.86 0.5\n",
      "[[37905     0]\n",
      " [ 6095     0]] 0.86 0.5\n",
      "[[38342     0]\n",
      " [ 5658     0]] 0.87 0.5\n",
      "[[37783     0]\n",
      " [ 6217     0]] 0.86 0.5\n",
      "level 12\n",
      "[[37354     0]\n",
      " [ 6646     0]] 0.85 0.5\n",
      "[[37574     0]\n",
      " [ 6426     0]] 0.85 0.5\n",
      "[[37877     0]\n",
      " [ 6123     0]] 0.86 0.5\n",
      "[[37401     0]\n",
      " [ 6599     0]] 0.85 0.5\n",
      "level 13\n",
      "[[37059     0]\n",
      " [ 6941     0]] 0.84 0.5\n",
      "[[37162     0]\n",
      " [ 6838     0]] 0.84 0.5\n",
      "[[37451     0]\n",
      " [ 6549     0]] 0.85 0.5\n",
      "[[37082     0]\n",
      " [ 6918     0]] 0.84 0.5\n",
      "level 14\n",
      "[[36726     0]\n",
      " [ 7274     0]] 0.83 0.5\n",
      "[[36829     0]\n",
      " [ 7171     0]] 0.84 0.5\n",
      "[[37138     0]\n",
      " [ 6862     0]] 0.84 0.5\n",
      "[[36760     0]\n",
      " [ 7240     0]] 0.84 0.5\n",
      "level 15\n",
      "[[36559     0]\n",
      " [ 7441     0]] 0.83 0.5\n",
      "[[36345     0]\n",
      " [ 7655     0]] 0.83 0.5\n",
      "[[36720     0]\n",
      " [ 7280     0]] 0.83 0.5\n",
      "[[36442     0]\n",
      " [ 7558     0]] 0.83 0.5\n",
      "level 16\n",
      "[[36255     0]\n",
      " [ 7745     0]] 0.82 0.5\n",
      "[[36204     0]\n",
      " [ 7796     0]] 0.82 0.5\n",
      "[[36582     0]\n",
      " [ 7418     0]] 0.83 0.5\n",
      "[[36141     0]\n",
      " [ 7859     0]] 0.82 0.5\n",
      "level 17\n",
      "[[36174     0]\n",
      " [ 7826     0]] 0.82 0.5\n",
      "[[35899     0]\n",
      " [ 8101     0]] 0.82 0.5\n",
      "[[36325     0]\n",
      " [ 7675     0]] 0.83 0.5\n",
      "[[36056     0]\n",
      " [ 7944     0]] 0.82 0.5\n",
      "level 18\n",
      "[[35967     0]\n",
      " [ 8033     0]] 0.82 0.5\n",
      "[[35671     0]\n",
      " [ 8329     0]] 0.81 0.5\n",
      "[[36287     0]\n",
      " [ 7713     0]] 0.82 0.5\n",
      "[[35813     0]\n",
      " [ 8187     0]] 0.81 0.5\n",
      "level 19\n",
      "[[35815     0]\n",
      " [ 8185     0]] 0.81 0.5\n",
      "[[35491     0]\n",
      " [ 8509     0]] 0.81 0.5\n",
      "[[35935     0]\n",
      " [ 8065     0]] 0.82 0.5\n",
      "[[35734     0]\n",
      " [ 8266     0]] 0.81 0.5\n",
      "\n",
      " RANDOM MODEL: \n",
      "\n",
      "level 0\n",
      "[[20157 20116]\n",
      " [ 1858  1869]] 0.5 0.5\n",
      "[[19977 19979]\n",
      " [ 1992  2052]] 0.5 0.5\n",
      "[[20371 19925]\n",
      " [ 1847  1857]] 0.51 0.5\n",
      "[[20040 20002]\n",
      " [ 2019  1939]] 0.5 0.5\n",
      "level 1\n",
      "[[20070 19936]\n",
      " [ 1988  2006]] 0.5 0.5\n",
      "[[19849 19964]\n",
      " [ 2076  2111]] 0.5 0.5\n",
      "[[19844 20290]\n",
      " [ 1963  1903]] 0.49 0.49\n",
      "[[19933 20002]\n",
      " [ 2019  2046]] 0.5 0.5\n",
      "level 2\n",
      "[[19914 19963]\n",
      " [ 2086  2037]] 0.5 0.5\n",
      "[[19969 19793]\n",
      " [ 2084  2154]] 0.5 0.51\n",
      "[[20069 20005]\n",
      " [ 1964  1962]] 0.5 0.5\n",
      "[[20040 19826]\n",
      " [ 2111  2023]] 0.5 0.5\n",
      "level 3\n",
      "[[19823 19898]\n",
      " [ 2165  2114]] 0.5 0.5\n",
      "[[19668 19981]\n",
      " [ 2119  2232]] 0.5 0.5\n",
      "[[20017 20019]\n",
      " [ 1984  1980]] 0.5 0.5\n",
      "[[19932 19855]\n",
      " [ 2044  2169]] 0.5 0.51\n",
      "level 4\n",
      "[[19776 19801]\n",
      " [ 2205  2218]] 0.5 0.5\n",
      "[[19829 19689]\n",
      " [ 2248  2234]] 0.5 0.5\n",
      "[[20038 19878]\n",
      " [ 2013  2071]] 0.5 0.5\n",
      "[[19818 19902]\n",
      " [ 2148  2132]] 0.5 0.5\n",
      "level 5\n",
      "[[19830 19707]\n",
      " [ 2264  2199]] 0.5 0.5\n",
      "[[19842 19538]\n",
      " [ 2275  2345]] 0.5 0.51\n",
      "[[19926 19793]\n",
      " [ 2141  2140]] 0.5 0.5\n",
      "[[19628 19998]\n",
      " [ 2168  2206]] 0.5 0.5\n",
      "level 6\n",
      "[[19584 19728]\n",
      " [ 2391  2297]] 0.5 0.49\n",
      "[[19766 19570]\n",
      " [ 2330  2334]] 0.5 0.5\n",
      "[[19856 19747]\n",
      " [ 2229  2168]] 0.5 0.5\n",
      "[[19723 19776]\n",
      " [ 2236  2265]] 0.5 0.5\n",
      "level 7\n",
      "[[19562 19506]\n",
      " [ 2443  2489]] 0.5 0.5\n",
      "[[19633 19474]\n",
      " [ 2487  2406]] 0.5 0.5\n",
      "[[19551 19922]\n",
      " [ 2349  2178]] 0.49 0.49\n",
      "[[19590 19594]\n",
      " [ 2418  2398]] 0.5 0.5\n",
      "level 8\n",
      "[[19583 19252]\n",
      " [ 2573  2592]] 0.5 0.5\n",
      "[[19514 19370]\n",
      " [ 2605  2511]] 0.5 0.5\n",
      "[[19493 19814]\n",
      " [ 2332  2361]] 0.5 0.5\n",
      "[[19342 19569]\n",
      " [ 2570  2519]] 0.5 0.5\n",
      "level 9\n",
      "[[19244 19278]\n",
      " [ 2739  2739]] 0.5 0.5\n",
      "[[19248 19295]\n",
      " [ 2737  2720]] 0.5 0.5\n",
      "[[19778 19391]\n",
      " [ 2389  2442]] 0.5 0.51\n",
      "[[19398 19275]\n",
      " [ 2628  2699]] 0.5 0.5\n",
      "level 10\n",
      "[[18947 19294]\n",
      " [ 2837  2922]] 0.5 0.5\n",
      "[[19173 19059]\n",
      " [ 2895  2873]] 0.5 0.5\n",
      "[[19359 19409]\n",
      " [ 2617  2615]] 0.5 0.5\n",
      "[[19284 18979]\n",
      " [ 2841  2896]] 0.5 0.5\n",
      "level 11\n",
      "[[18972 18882]\n",
      " [ 3081  3065]] 0.5 0.5\n",
      "[[19154 18751]\n",
      " [ 3030  3065]] 0.5 0.5\n",
      "[[19139 19203]\n",
      " [ 2835  2823]] 0.5 0.5\n",
      "[[18944 18839]\n",
      " [ 3097  3120]] 0.5 0.5\n",
      "level 12\n",
      "[[18479 18875]\n",
      " [ 3322  3324]] 0.5 0.5\n",
      "[[18669 18905]\n",
      " [ 3199  3227]] 0.5 0.5\n",
      "[[18892 18985]\n",
      " [ 3095  3028]] 0.5 0.5\n",
      "[[18796 18605]\n",
      " [ 3321  3278]] 0.5 0.5\n",
      "level 13\n",
      "[[18595 18464]\n",
      " [ 3499  3442]] 0.5 0.5\n",
      "[[18617 18545]\n",
      " [ 3425  3413]] 0.5 0.5\n",
      "[[18814 18637]\n",
      " [ 3208  3341]] 0.5 0.51\n",
      "[[18689 18393]\n",
      " [ 3479  3439]] 0.5 0.5\n",
      "level 14\n",
      "[[18293 18433]\n",
      " [ 3718  3556]] 0.5 0.49\n",
      "[[18281 18548]\n",
      " [ 3636  3535]] 0.5 0.49\n",
      "[[18757 18381]\n",
      " [ 3401  3461]] 0.5 0.5\n",
      "[[18548 18212]\n",
      " [ 3619  3621]] 0.5 0.5\n",
      "level 15\n",
      "[[18403 18156]\n",
      " [ 3707  3734]] 0.5 0.5\n",
      "[[18189 18156]\n",
      " [ 3770  3885]] 0.5 0.5\n",
      "[[18347 18373]\n",
      " [ 3646  3634]] 0.5 0.5\n",
      "[[18416 18026]\n",
      " [ 3684  3874]] 0.51 0.51\n",
      "level 16\n",
      "[[18210 18045]\n",
      " [ 3842  3903]] 0.5 0.5\n",
      "[[17847 18357]\n",
      " [ 3950  3846]] 0.49 0.49\n",
      "[[18206 18376]\n",
      " [ 3643  3775]] 0.5 0.5\n",
      "[[18197 17944]\n",
      " [ 3887  3972]] 0.5 0.5\n",
      "level 17\n",
      "[[18080 18094]\n",
      " [ 3918  3908]] 0.5 0.5\n",
      "[[17975 17924]\n",
      " [ 4195  3906]] 0.5 0.49\n",
      "[[17970 18355]\n",
      " [ 3865  3810]] 0.5 0.5\n",
      "[[17961 18095]\n",
      " [ 4039  3905]] 0.5 0.49\n",
      "level 18\n",
      "[[17949 18018]\n",
      " [ 3884  4149]] 0.5 0.51\n",
      "[[17892 17779]\n",
      " [ 4173  4156]] 0.5 0.5\n",
      "[[18053 18234]\n",
      " [ 3893  3820]] 0.5 0.5\n",
      "[[17902 17911]\n",
      " [ 4100  4087]] 0.5 0.5\n",
      "level 19\n",
      "[[17824 17991]\n",
      " [ 4104  4081]] 0.5 0.5\n",
      "[[17766 17725]\n",
      " [ 4176  4333]] 0.5 0.5\n",
      "[[18021 17914]\n",
      " [ 4028  4037]] 0.5 0.5\n",
      "[[18013 17721]\n",
      " [ 4150  4116]] 0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "\n",
    "import random\n",
    "random.seed(7)\n",
    "\n",
    "\n",
    "def generator(X, Y, batch_size=32, train=True):\n",
    "    while True:\n",
    "        for offset in range(0, len(X), batch_size):\n",
    "            X_batch = np.stack(X[offset:offset+batch_size], axis=0)\n",
    "            Y_batch = np.stack(Y[offset:offset+batch_size], axis=0)\n",
    "\n",
    "            Y_batch_ = np.empty((Y_batch.shape[0], 2,2,20))\n",
    "            for m in range(Y_batch.shape[0]):\n",
    "                for i in range(20):\n",
    "                    Y_batch_[m, :,:,i] = [[np.sum(Y_batch[m, :10, :10, i]), np.sum(Y_batch[m, :10, 10:, i])], [np.sum(Y_batch[m, 10:, :10, i]), np.sum(Y_batch[m, 10:, 10:, i])]]\n",
    "            Y_batch_[Y_batch_ > 0] = 1\n",
    "\n",
    "            if train:\n",
    "                Y_f = np.array([Y_batch_[e].flatten() for e in range(Y_batch_.shape[0])])\n",
    "                yield (X_batch, Y_f)\n",
    "            else:\n",
    "                yield X_batch\n",
    "\n",
    "structure_ids = []\n",
    "for line in open('./structures lists/structures human.txt', 'r'):\n",
    "    line = line.strip('\\n')\n",
    "    structure_ids.append(line)\n",
    "# for line in open('./structures lists/structures ecoli.txt', 'r'):\n",
    "#     line = line.strip('\\n')\n",
    "#     structure_ids.append(line)\n",
    "structure_ids.remove('4pkd')\n",
    "structure_ids.remove('1a9n')\n",
    "structure_ids.remove('2adc')\n",
    "random.shuffle(structure_ids)\n",
    "print(len(structure_ids))\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "num_aa_train = 0\n",
    "num_aa_test = 0\n",
    "num_train = int(len(structure_ids)*0.7)\n",
    "for i, structure_id in enumerate(structure_ids):\n",
    "    protein = np.load('../data/voxelized data 20x20x20/' + structure_id + '_protein.npy', mmap_mode='r')\n",
    "    rna = np.load('../data/voxelized data 20x20x20/' + structure_id + '_rna_3D.npy', mmap_mode='r')\n",
    "    na = 0\n",
    "    pos = 0\n",
    "    while (np.sum(rna[na]) > 0) and (na < len(rna)-1):\n",
    "        pos +=1\n",
    "        na +=1\n",
    "    \n",
    "\n",
    "    if i <= num_train:\n",
    "        if pos > len(rna)/2:\n",
    "            X_train.extend(protein[:, :, :, :, :3])\n",
    "            Y_train.extend(rna)\n",
    "            num_aa_train +=len(rna)\n",
    "        else:\n",
    "            X_train.extend(protein[:pos, :, :, :, :3])\n",
    "            X_train.extend(protein[-pos:, :, :, :, :3])\n",
    "            Y_train.extend(rna[:pos])\n",
    "            Y_train.extend(rna[-pos:])\n",
    "            num_aa_train +=2*pos\n",
    "    else:\n",
    "        if pos > len(rna)/2:\n",
    "            X_test.extend(protein[:, :, :, :, :3])\n",
    "            Y_test.extend(rna)\n",
    "            num_aa_test +=len(rna)\n",
    "        else:\n",
    "            X_test.extend(protein[:pos, :, :, :, :3])\n",
    "            X_test.extend(protein[-pos:, :, :, :, :3])\n",
    "            Y_test.extend(rna[:pos])\n",
    "            Y_test.extend(rna[-pos:])\n",
    "            num_aa_test +=2*pos\n",
    "\n",
    "Y_test = np.stack(Y_test, axis=0)\n",
    "Y_test_ = np.empty((len(Y_test), 2,2,20))\n",
    "for m in range(len(Y_test)):\n",
    "    for i in range(20):\n",
    "        Y_test_[m, :,:,i] = [[np.sum(Y_test[m, :10, :10, i]), np.sum(Y_test[m, :10, 10:, i])], [np.sum(Y_test[m, 10:, :10, i]), np.sum(Y_test[m, 10:, 10:, i])]]\n",
    "Y_test_[Y_test_ > 0] = 1\n",
    "   \n",
    "n_steps_train = int(num_aa_train/400) \n",
    "n_steps_test = int(num_aa_test/400)\n",
    "\n",
    "print(num_aa_train, num_aa_test)\n",
    "\n",
    "generator_train = generator(X_train, Y_train, 400, True)\n",
    "generator_test = generator(X_test, Y_test, 400, False)\n",
    "\n",
    "ins = tf.keras.layers.Input((20, 20, 20, 3))\n",
    "con1 = tf.keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), padding='same', activation='relu')(ins)\n",
    "con2 = tf.keras.layers.Conv3D(filters=32, kernel_size=(3, 3, 3), padding='same', activation='relu')(con1)\n",
    "con3 = tf.keras.layers.Conv3D(filters=32, kernel_size=(3, 3, 3), padding='same', activation='relu')(con2)\n",
    "maxp1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2))(con3)\n",
    "con4 = tf.keras.layers.Conv3D(filters=32, kernel_size=(3, 3, 3), padding='same', activation='relu')(maxp1)\n",
    "con5 = tf.keras.layers.Conv3D(filters=16, kernel_size=(3, 3, 3), padding='same', activation='relu')(con4)\n",
    "con6 = tf.keras.layers.Conv3D(filters=16, kernel_size=(3, 3, 3), padding='same', activation='relu')(con5)\n",
    "maxp2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2))(con6)\n",
    "con7 = tf.keras.layers.Conv3D(filters=16, kernel_size=(3, 3, 3), padding='same', activation='relu')(maxp2)\n",
    "con8 = tf.keras.layers.Conv3D(filters=8, kernel_size=(3, 3, 3), padding='same', activation='relu')(con7)\n",
    "con9 = tf.keras.layers.Conv3D(filters=4, kernel_size=(3, 3, 3), padding='same', activation='relu')(con8)\n",
    "maxp3 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2))(con9)\n",
    "batch = tf.keras.layers.BatchNormalization()(maxp3)\n",
    "flat = tf.keras.layers.Flatten()(batch)\n",
    "dens2 = tf.keras.layers.Dense(units=256, activation='relu')(flat)\n",
    "drop2 = tf.keras.layers.Dropout(0.6)(dens2)\n",
    "outs = tf.keras.layers.Dense(units=80, activation='sigmoid')(drop2)\n",
    "model = tf.keras.models.Model(inputs=ins, outputs=outs)\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.00001), metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# checkpoint\n",
    "# filepath=\"weights_best.hdf5\"\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "# model.fit(X_train, Y_train_f, validation_split=0.33, epochs=1, batch_size=200, callbacks=callbacks_list, verbose=0)\n",
    "model.fit_generator(generator_train, steps_per_epoch=n_steps_train, epochs=100, callbacks=None, verbose=1, max_queue_size=2)\n",
    "\n",
    "# model_best = model\n",
    "# model_best.load_weights(\"weights_best.hdf5\")\n",
    "# print(model.evaluate(X_test, Y_test, verbose=0, batch_size=100))\n",
    "# model_best.save('model_cnn_15_2.h5')\n",
    "# Y_pred = model_best.predict(X_test, batch_size=200)\n",
    "Y_pred = model.predict_generator(generator_test, steps=n_steps_test)\n",
    "print(Y_pred.shape)\n",
    "Y_pred_ = np.array([Y_pred[i].reshape((2,2,20)) for i in range(Y_pred.shape[0])])\n",
    "\n",
    "#CNN\n",
    "Y_pred_[Y_pred_ >= 0.5] = 1\n",
    "Y_pred_[Y_pred_ < 0.5] = 0\n",
    "\n",
    "print(Y_pred_.shape)\n",
    "print(Y_test_.shape)\n",
    "Y_test_ = Y_test_[:Y_pred_.shape[0]]\n",
    "\n",
    "print('CNN: \\n')\n",
    "for i in range(20):\n",
    "    confusion_matrix = [sklearn.metrics.confusion_matrix(Y_test_[:,l , c, i], Y_pred_[:,l , c, i]) for l in range(2) for c in range(2)]\n",
    "    accuracy = [np.sum(np.trace(cm))/np.sum(cm) for cm in confusion_matrix]\n",
    "    auc = [sklearn.metrics.roc_auc_score(Y_test_[:,l , c, i], Y_pred_[:,l , c, i]) for l in range(2) for c in range(2)]\n",
    "\n",
    "    print(f'level {i}')\n",
    "    for q in range(len(confusion_matrix)):\n",
    "        print(confusion_matrix[q], np.round(accuracy[q], 2), np.round(auc[q], 2))\n",
    "\n",
    "# baseline model\n",
    "# predict all zeros; at least 50% correct predictions because there are 1/2 of negative examples\n",
    "Y_pred_base = np.zeros(Y_test_.shape)\n",
    "\n",
    "# po = np.sum(Y_train, axis=0)/Y_train.shape[0]\n",
    "# po[po >= 0.5] = 1\n",
    "# po[po < 0.5] = 0\n",
    "# Y_pred_base = np.tile(po, (Y_test.shape[0],1))\n",
    "\n",
    "Y_pred_base[Y_pred_base >= 0.5] = 1\n",
    "Y_pred_base[Y_pred_base < 0.5] = 0\n",
    "print(Y_pred_base.shape)\n",
    "print(f'\\n BASELINE MODEL: \\n')\n",
    "for i in range(20):\n",
    "    confusion_matrix_base = [sklearn.metrics.confusion_matrix(Y_test_[:,l, c, i], Y_pred_base[:,l, c, i]) for l in range(2) for c in range(2)]\n",
    "    accuracy_base = [np.sum(np.trace(cm))/np.sum(cm) for cm in confusion_matrix_base]\n",
    "    auc_base = [sklearn.metrics.roc_auc_score(Y_test_[:,l, c, i], Y_pred_base[:,l, c, i]) for l in range(2) for c in range(2)]\n",
    "    \n",
    "    print(f'level {i}')\n",
    "    for q in range(len(confusion_matrix_base)):\n",
    "        print(confusion_matrix_base[q], np.round(accuracy_base[q], 2), np.round(auc_base[q], 2))\n",
    "\n",
    "#random model\n",
    "Y_pred_random = np.random.random(Y_test_.shape)\n",
    "Y_pred_random[Y_pred_random >= 0.5] = 1\n",
    "Y_pred_random[Y_pred_random < 0.5] = 0\n",
    "\n",
    "print(f'\\n RANDOM MODEL: \\n')\n",
    "for i in range(20):\n",
    "    confusion_matrix_random = [sklearn.metrics.confusion_matrix(Y_test_[:,l, c, i], Y_pred_random[:,l, c, i]) for l in range(2) for c in range(2)]\n",
    "    accuracy_random = [np.sum(np.trace(cm))/np.sum(cm) for cm in confusion_matrix_random]\n",
    "    auc_random = [sklearn.metrics.roc_auc_score(Y_test_[:,l, c, i], Y_pred_random[:,l, c, i]) for l in range(2) for c in range(2)]\n",
    "\n",
    "    print(f'level {i}')\n",
    "    for q in range(len(confusion_matrix_random)):\n",
    "        print(confusion_matrix_random[q], np.round(accuracy_random[q], 2), np.round(auc_random[q], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
